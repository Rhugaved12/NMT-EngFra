{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchmetrics"
      ],
      "metadata": {
        "id": "J2gS0ZmCzzir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JklekWz9_0f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torchmetrics.functional import sacre_bleu_score\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "# https://download.pytorch.org/tutorial/data.zip"
      ],
      "metadata": {
        "id": "bYLZW9jbAJuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding text based on index. I.e one hot encoding for each unique word\n",
        "\n",
        "# start and end of sentence tokens\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1:\"EOS\"}\n",
        "        self.n_words = 2  #current no of words is 2, SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for w in sentence.split(\" \"):\n",
        "            self.addWord(w)\n",
        "\n",
        "    def addWord(self, w):\n",
        "        self.word2index[w] = self.word2index.get(w, self.n_words)\n",
        "        # self.word2count[w] = 1\n",
        "        self.index2word[self.n_words] = w\n",
        "        self.n_words += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "OgMaZ7ElAhT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COnvert unicode to ascii\n",
        "def unicodeToAscii(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != \"Mn\")\n",
        "\n",
        "\n",
        "# lowercase, trim and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "4PxYd0pcGNCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the file into lines, and then split the lines into pairs of languages\n",
        "# The reverse flag is used for french to english translation\n",
        "\n",
        "def getSeparetedData(lang1, lang2, reverse=False):\n",
        "    \n",
        "    text_file = open(\"%s-%s.txt\"  % (lang1, lang2), \"r\", encoding='utf-8')\n",
        "    lines = text_file.read().strip().split('\\n')\n",
        "\n",
        "    # split on tab\n",
        "    pairs = [[normalizeString(s) for s in line.split('\\t')] for line in lines]\n",
        "    print(pairs[0:10])\n",
        "\n",
        "    # if reverse is true, reverse the pairs\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        in_lang = Lang(lang2)\n",
        "        out_lang = Lang(lang1) \n",
        "    else:\n",
        "        in_lang = Lang(lang1)\n",
        "        out_lang = Lang(lang2)\n",
        "\n",
        "    return in_lang, out_lang, pairs"
      ],
      "metadata": {
        "id": "dp30RKyqJ68H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "D5ETmuSaJ9eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "\n",
        "    # read the file and separete the data and create Lang objects\n",
        "    in_lang, out_lang, pairs = getSeparetedData(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        in_lang.addSentence(pair[0])\n",
        "        out_lang.addSentence(pair[1])\n",
        "    print(\"Counted words.\")\n",
        "    print(in_lang.name, in_lang.n_words)\n",
        "    print(out_lang.name, out_lang.n_words)\n",
        "\n",
        "    return in_lang, out_lang, pairs\n"
      ],
      "metadata": {
        "id": "dVgSC2ViPnIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_lang, out_lang, pairs = prepareData('eng', 'fra')\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaJDOBOvRfP0",
        "outputId": "bca60e90-a002-49fe-c069-bf701d113e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !'], ['help !', 'a l aide !'], ['jump .', 'saute .'], ['stop !', 'ca suffit !'], ['stop !', 'stop !'], ['stop !', 'arrete toi !']]\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words.\n",
            "eng 63973\n",
            "fra 65674\n",
            "['we re going to need some help .', 'nous allons avoir besoin d aide .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YUB-RHNgyFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # reshape embedding vector as batch size=1, sequence length=1 and embedding size=embedding_size \n",
        "        embedded = self.embedding(input)\n",
        "        # print(\"ENCODER: \")\n",
        "        # print(embedded.shape)\n",
        "        # the GRU layer that comes next expects an input of size [sequence_length, batch_size, input_size]\n",
        "        output = embedded.view(1, 1, -1)\n",
        "        \n",
        "        # print(output.shape, hidden.shape)\n",
        "        # print(input.shape)\n",
        "        # print(input)\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "L5IftaXKVaCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # reshape embedding vector as batch size=1, sequence length=1 and embedding size=embedding_size \n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(embedded)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "p8lgDlMWZZr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention class\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "9P13P9lneaXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPairs(pair):\n",
        "    in_tensor = tensorFromSentence(in_lang, pair[0])\n",
        "    out_tensor = tensorFromSentence(out_lang, pair[1])\n",
        "    return (in_tensor, out_tensor)    "
      ],
      "metadata": {
        "id": "29nPpeaViRzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the train function\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(in_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    in_length = in_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    # print(\"in_length, target_length\", in_length, target_length)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "    \n",
        "    for i in range(in_length):\n",
        "        # print(in_tensor[i].shape, encoder_hidden.shape)\n",
        "        encoder_output, encoder_hidden = encoder(in_tensor[i], encoder_hidden)\n",
        "        encoder_outputs[i] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for i in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[i])\n",
        "            decoder_input = target_tensor[i]    #teacher forcing\n",
        "\n",
        "    else:\n",
        "        for i in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  #detach from history as input     \n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[i])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()/target_length\n",
        "\n"
      ],
      "metadata": {
        "id": "QrSCtP5kjo1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to keep tarck of time\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "-HgcLqOFksEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPairs(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "G9T5xPlvnLo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bs1rjwYrpngz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in_lang.word2index"
      ],
      "metadata": {
        "id": "P6UdiAmqqH0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = Encoder(in_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = AttnDecoderRNN(hidden_size, out_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 125000, print_every=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPyGW0iPptQ8",
        "outputId": "54b8d210-a104-4f19-d1e5-2c9d835afa85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3m 17s (- 79m 9s) (5000 4%) 3.9083\n",
            "6m 28s (- 74m 29s) (10000 8%) 3.0756\n",
            "9m 41s (- 71m 1s) (15000 12%) 2.6686\n",
            "12m 53s (- 67m 43s) (20000 16%) 2.4171\n",
            "16m 7s (- 64m 28s) (25000 20%) 2.1872\n",
            "19m 20s (- 61m 13s) (30000 24%) 1.9575\n",
            "22m 34s (- 58m 3s) (35000 28%) 1.8147\n",
            "25m 49s (- 54m 52s) (40000 32%) 1.6764\n",
            "29m 3s (- 51m 39s) (45000 36%) 1.5600\n",
            "32m 17s (- 48m 26s) (50000 40%) 1.4263\n",
            "35m 31s (- 45m 13s) (55000 44%) 1.3296\n",
            "38m 45s (- 41m 59s) (60000 48%) 1.2301\n",
            "41m 59s (- 38m 45s) (65000 52%) 1.1642\n",
            "45m 15s (- 35m 33s) (70000 56%) 1.0862\n",
            "48m 31s (- 32m 20s) (75000 60%) 1.0267\n",
            "51m 47s (- 29m 7s) (80000 64%) 0.9951\n",
            "55m 2s (- 25m 54s) (85000 68%) 0.9480\n",
            "58m 17s (- 22m 40s) (90000 72%) 0.8958\n",
            "61m 32s (- 19m 26s) (95000 76%) 0.8741\n",
            "64m 47s (- 16m 11s) (100000 80%) 0.8083\n",
            "68m 2s (- 12m 57s) (105000 84%) 0.8163\n",
            "71m 18s (- 9m 43s) (110000 88%) 0.7426\n",
            "74m 33s (- 6m 28s) (115000 92%) 0.7682\n",
            "77m 47s (- 3m 14s) (120000 96%) 0.7338\n",
            "81m 1s (- 0m 0s) (125000 100%) 0.7438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(in_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(out_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "metadata": {
        "id": "egrFUuZ4q_ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=1000):\n",
        "    # calculate the bleu score\n",
        "    # gold_sents is a list of lists because every gold sentence may have multiple translations \n",
        "    gold_sents = []\n",
        "    # pred_sents in a list\n",
        "    pred_sents = []\n",
        "\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        gold_sents.append([pair[1]])\n",
        "\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        pred_sents.append(output_sentence)\n",
        "\n",
        "    bleu_score = sacre_bleu_score(pred_sents, gold_sents)\n",
        "    print(\"BLEU Score = \", bleu_score, end='\\n\\n')\n",
        "\n",
        "    for i in range(10):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "EomD8Kpb8FrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, decoder1, 5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rb75B1g8IUl",
        "outputId": "03f14205-5949-4f0f-dead-02d8516518d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score =  tensor(0.3398)\n",
            "\n",
            "> we re winning .\n",
            "= nous sommes en train de gagner .\n",
            "< nous sommes en train de l embrasser . <EOS>\n",
            "\n",
            "> i m skinny .\n",
            "= je suis maigrichon .\n",
            "< je suis maigrichonne . <EOS>\n",
            "\n",
            "> she is a poor cook .\n",
            "= c est une pietre cuisiniere .\n",
            "< c est une mauvaise d elite . <EOS>\n",
            "\n",
            "> he s leaning on a cane .\n",
            "= il s appuie sur une canne .\n",
            "< il s une une une . . <EOS>\n",
            "\n",
            "> i m thirty years older than you .\n",
            "= j ai trente ans de plus que vous .\n",
            "< je ai trente ans de plus que vous . <EOS>\n",
            "\n",
            "> they re disposable .\n",
            "= ils sont jetables .\n",
            "< ils sont jetables . <EOS>\n",
            "\n",
            "> i am very busy these days .\n",
            "= je suis tres occupe ces jours ci .\n",
            "< je suis tres occupe ces jours ces jours . <EOS>\n",
            "\n",
            "> you re alone aren t you ?\n",
            "= vous etes seules n est ce pas ?\n",
            "< tu es seule n est ce pas ? <EOS>\n",
            "\n",
            "> i m looking forward to seeing you soon .\n",
            "= j ai hate de te voir bientot .\n",
            "< je me impatiente de te voir bientot . <EOS>\n",
            "\n",
            "> i am very dangerous .\n",
            "= je suis vraiment dangereux .\n",
            "< je suis tres dangereux . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define file paths to save and load the models\n",
        "encoder_path = 'NMTencoder.pth'\n",
        "decoder_path = 'NMTdecoder_Att.pth'\n",
        "\n",
        "# save the encoder1 model\n",
        "torch.save(encoder1.state_dict(), encoder_path)\n",
        "\n",
        "# save the decoder1 model\n",
        "torch.save(decoder1.state_dict(), decoder_path)\n",
        "\n",
        "# load the saved models\n",
        "# loaded_encoder = Encoder(in_lang.n_words, hidden_size).to(device)\n",
        "# loaded_encoder.load_state_dict(torch.load(encoder_path))\n",
        "\n",
        "# loaded_decoder = AttnDecoderRNN(hidden_size, out_lang.n_words, dropout_p=0.1).to(device)\n",
        "# loaded_decoder.load_state_dict(torch.load(decoder_path))"
      ],
      "metadata": {
        "id": "uC7tfNcnmBEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "NvapjCqY8W5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d6b327-8c00-475e-decc-7924cbf69fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_weights_path = '/content/drive/My Drive/NMT_Weights/my_model_weights.pt'\n",
        "# torch.save(model.state_dict(), model_weights_path)\n",
        "\n",
        "\n",
        "encoder_path = '/content/drive/My Drive/NMT_Weights/New_NMT_EngToGer_encoder.pth'\n",
        "decoder_path = '/content/drive/My Drive/NMT_Weights/New_NMT_EngToGer_decoder_Att.pth'\n",
        "\n",
        "# save the encoder1 model\n",
        "torch.save(encoder1.state_dict(), encoder_path)\n",
        "\n",
        "# save the decoder1 model\n",
        "torch.save(decoder1.state_dict(), decoder_path)"
      ],
      "metadata": {
        "id": "J1I_y7kpqlFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtauGkeHrQWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}